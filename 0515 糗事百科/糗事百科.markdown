'''# -*- coding:utf-8 -*-
import urllib2
from bs4 import BeautifulSoup#python3用bs3
import threading

def getPage(Url):#获得html文件方法
    #定义User-Agent
    userAgent ='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110'
    headers = {'User-Agent':userAgent}#装进头文件
    req = urllib2.Request(url=Url,headers=headers)#合成 请求命令()
    try:#预防503错误——服务器错误()
        response = urllib2.urlopen(req)#发送 请求命令 并获得 html 文件()
        content = response.read()#html 内容读取

    except Exception,e:
        content = None
        print('nothing')
    return content #获取内容(html 文件内容)

Url1='http://www.qiushibaike.com/8hr/page/%d'
comUrl = 'http://www.qiushibaike.com/article/%s'
page = 0
getPage(Url1 % page)

while page !=10:#获取1-10页
    page +=1 #迭代页码
    Url = Url1 % page #url 合成
    #获得ID,用来去文章所在网站
    idListPage = getPage(Url)#获得 article目录 html文件内容
    soupA= BeautifulSoup(idListPagea,'lxml') #解析html(???)
    List = soupA.find_all(attrs='article block untagged mb15')#生成一个list,
    for string in List:
        articleId = str(string.get('id')).strip()[11:]


###知识点:
''' 1.response status_code -> evernote http/1.1

    2.try ,except 功能测试成功再加上去

###question:
    1.q = Soup1.find_all('a')
        q是什么格式? tuple? list? dict? str?





'''
![求](/Users/colinlaung/Desktop/d.png)
'''